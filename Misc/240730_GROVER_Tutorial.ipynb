{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azTfbLsebKly"
      },
      "source": [
        "# **Tutorial GROVER - DNA Language Model**\n",
        "\n",
        "Melissa Sanabria, Jonas Hirsch, Pierre M. Joubert, Anna R. Poetsch\n",
        "\n",
        "Biomedical Genomics, Biotechnology Center, Center for Molecular and Cellular Bioengineering, Technische Universit√§t Dresden  \n",
        "melissa.sanabria@tu-dresden.de arpoetsch@gmail.com\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEYEy5gpTuq4"
      },
      "source": [
        "# Overview\n",
        "\n",
        "This tutorial will show you how to use the DNA language model GROVER to perform fine-tuning tasks to investigate genome biology.\n",
        "\n",
        "[GROVER](https://www.nature.com/articles/s42256-024-00872-0) (\"Genome Rules Obtained Via Extracted Representations\") is a foundation DNA language model with an optimized vocabulary for the human genome. It has been pre-trained on the human genome and needs to be trained a second time, or fine-tuned, to perform specific tasks.\n",
        "\n",
        "For this tutorial we chose an end-to-end example to fine-tune the pre-tained GROVER to predict DNA binding by the CCCTC-Binding Factor (CTCF). The task is to recognize which sites that contain a CTCF binding motif are actually bound by the protein. We will be using ChIP-seq data from HepG2 cells obtained from ENCODE.  \n",
        "\n",
        "Throughout this tutorial, you will find ***MODIFY*** signs that indicate pieces of code that are task specific, meaning that they are specific for CTCF binding site prediction, and you can modify them for your desired task.\n",
        "\n",
        "**WARNING**: Even though Google Colab gives you access to a GPU, which is necessary for this computationally expensive work, the free version only gives you access to a very simple device and the training might take a while.\n",
        "\n",
        "In the version of Colab, which is free of charge, notebooks can run for a maximum of 12 hours, depending on availability and your usage patterns.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tLHIRp6TxbR"
      },
      "source": [
        "## Setup\n",
        "\n",
        "First, save a copy of this notebook in your Google Drive by navigating to 'File' then 'Save a copy in Drive...'.\n",
        "\n",
        "Once you've opened your own copy, make sure you have enabled the GPU runtime for Google Colab by navigating to the menu 'Runtime', select 'Change runtime type' and set the runtime to 'T4 GPU'.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5IDLhQzWLCe"
      },
      "source": [
        "## Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YE8H1rrWWVxQ"
      },
      "outputs": [],
      "source": [
        "!pip install wget"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MX13rpTxUOtc"
      },
      "source": [
        "## Import necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeDpnVpsRW-v"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import wget\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import matthews_corrcoef, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, Trainer, AutoModelForSequenceClassification, TrainingArguments\n",
        "from torch.utils.data import Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCq5uQdrRW-y"
      },
      "source": [
        "# Prepare Data\n",
        "\n",
        "**MODIFY!**\n",
        "\n",
        "This section is task dependent.\n",
        "\n",
        "We have listed each CTCF motif in the genome which has been detected by the FIMO software. For this specific task, we calculate the center of the motif and then add 500 nucleotides up- and downstream. This is to capture the sequence context of the motif, which is what GROVER was trained to work with."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create dataset\n",
        "\n",
        "From now on, we come back to general instructions that apply for any fine-tuning task.\n",
        "\n",
        "Please remember that to do this yourself you will need a data frame with at least two columns: label, sequence"
      ],
      "metadata": {
        "id": "dTRzJAc2ZFVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CTCF_dataset_url = \"https://datashare.tu-dresden.de/s/pjb5gcWGGrbcARN/download/CTCF_dataset.tsv\"\n",
        "file_name = \"CTCF_dataset.tsv\"\n",
        "if os.path.exists(file_name):\n",
        "  os.remove(file_name) # if exists, remove it directly\n",
        "\n",
        "print(\"Starting downloading\")\n",
        "file_name = wget.download(CTCF_dataset_url, out=file_name)\n",
        "print(file_name)"
      ],
      "metadata": {
        "id": "FKRowQ5GY4b6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CTCF_dataset = pd.read_csv('CTCF_dataset.tsv', sep='\\t')\n",
        "CTCF_dataset"
      ],
      "metadata": {
        "id": "YJr-DtbYZaWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After getting our dataset, we need to split the samples into train, validation and test. We will go for the standard partitions, 80% for training, 10% for testing and 10% for validation."
      ],
      "metadata": {
        "id": "OMSntPD-a4aa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = CTCF_dataset.sample(frac=0.8, random_state=0)\n",
        "validation = CTCF_dataset.drop(train.index)\n",
        "test = validation.sample(frac=0.5, random_state=0)\n",
        "validation = validation.drop(test.index)\n",
        "\n",
        "train = train.reset_index(drop=True)\n",
        "validation = validation.reset_index(drop=True)\n",
        "test = test.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "L0nvyRp2bCLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GROVER in action"
      ],
      "metadata": {
        "id": "mOZzimS0bN0N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, we have not mentioned any Language Model terminology. Now, we need to change our sequence from nucleotides to tokens (or words). For this, we will download the tokenizer available in the higgingface project for GROVER. For this, we can use the model name *PoetschLab/GROVER*"
      ],
      "metadata": {
        "id": "Ctsbs6iDZuFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedTokenizerFast"
      ],
      "metadata": {
        "id": "PMAT3KasqoEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"PoetschLab/GROVER\")"
      ],
      "metadata": {
        "id": "VsKECaMnany1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we need to download the pre-trained GROVER model."
      ],
      "metadata": {
        "id": "q91zgtv8bXEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForMaskedLM.from_pretrained(\"PoetschLab/GROVER\")"
      ],
      "metadata": {
        "id": "HE1FPPeuMj9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to make your work easier, we create a class to process the Dataset created for Grover.\n"
      ],
      "metadata": {
        "id": "-J2_TXikc6CO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SupervisedDataset(Dataset):\n",
        "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
        "\n",
        "    def __init__(self, texts, labels, tokenizer):\n",
        "\n",
        "        super(SupervisedDataset, self).__init__()\n",
        "\n",
        "        sequences = [text for text in texts]\n",
        "\n",
        "        output = tokenizer(\n",
        "            sequences,\n",
        "            add_special_tokens=True,\n",
        "            max_length=310,\n",
        "            padding=\"longest\",\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        self.input_ids = output[\"input_ids\"]\n",
        "        self.attention_mask = output[\"attention_mask\"]\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return dict(\n",
        "            input_ids=self.input_ids[i],\n",
        "            labels=self.labels[i],\n",
        "            attention_mask=self.attention_mask[i]\n",
        "        )"
      ],
      "metadata": {
        "id": "AioLzAJpc6-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = SupervisedDataset(train.sequence, train.label, tokenizer)\n",
        "test_dataset = SupervisedDataset(test.sequence, test.label, tokenizer)\n",
        "val_dataset = SupervisedDataset(validation.sequence, validation.label, tokenizer)"
      ],
      "metadata": {
        "id": "PFzOXFxBdTwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train model\n",
        "\n",
        "For the training process we choose:\n",
        "\n",
        "*   an Adam optimizer but you can explore the [optimizers](https://huggingface.co/transformers/v3.0.2/main_classes/optimizer_schedules.html) available in huggingface\n",
        "*   learning rate of 0.000001 since for fine-tuning tasks it is better to have a very low learning rate\n",
        "*   a total number of epochs of 4\n",
        "\n",
        "A higher number of epochs is ideal but Colab limits the amount of time you can run some code. We advice you to try with your own GPU resources with at least 10 epochs.\n",
        "\n",
        "In order to make your work easier, we create a method to compute five classification metrics. Here we will see some metrics such as accuracy, f1 score, precision and recall. You can explore the [sklearn metrics](https://scikit-learn.org/stable/modules/model_evaluation.html) to choose other options."
      ],
      "metadata": {
        "id": "Bmty5erKdk9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metric_with_sklearn(logits: np.ndarray, labels: np.ndarray):\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, predictions),\n",
        "        \"f1\": f1_score(\n",
        "            labels, predictions, average=\"macro\", zero_division=0\n",
        "        ),\n",
        "        \"matthews_correlation\": matthews_corrcoef(\n",
        "            labels, predictions\n",
        "        ),\n",
        "        \"precision\": precision_score(\n",
        "            labels, predictions, average=\"macro\", zero_division=0\n",
        "        ),\n",
        "        \"recall\": recall_score(\n",
        "            labels, predictions, average=\"macro\", zero_division=0\n",
        "        ),\n",
        "    }\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    if isinstance(logits, tuple):  # Unpack logits if it's a tuple\n",
        "        logits = logits[0]\n",
        "    return calculate_metric_with_sklearn(logits, labels)"
      ],
      "metadata": {
        "id": "yuVwKF-2hP0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_args = TrainingArguments(seed = 42,\n",
        "                               output_dir=\".\",\n",
        "                               per_device_train_batch_size=16,\n",
        "                               eval_strategy=\"epoch\",\n",
        "                               learning_rate=0.000001,\n",
        "                               num_train_epochs=4\n",
        "                               )\n",
        "trainer = transformers.Trainer(\n",
        "                                model=model,\n",
        "                                tokenizer=tokenizer,\n",
        "                                compute_metrics=compute_metrics,\n",
        "                                train_dataset=train_dataset,\n",
        "                                eval_dataset=val_dataset,\n",
        "                                args = train_args\n",
        "                                )\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "0tTyIXo-dlNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test model\n",
        "\n",
        "After training the model, we can see the performance of our model on the test set, which are samples that the model has not previously seen."
      ],
      "metadata": {
        "id": "1Hxq6SMZh_5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = trainer.evaluate(eval_dataset=test_dataset)"
      ],
      "metadata": {
        "id": "D6dGSxPviBXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "ayrhgrZliZ55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now, it's your turn\n",
        "\n",
        "We use CTCF binding site as example but you can also use it for many other tasks such as promoter prediction, structural variants, integration sites of transposable elements, other binding sites, and many more."
      ],
      "metadata": {
        "id": "XEieilNqidoB"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}